point(0, 0, type = "*", pch = 20)
points(0, 0, type = "*", pch = 20)
points(0, 0, type = "*", col = "blue", pch = 20)
points(0, 0, cex=15, col = "blue", pch = 20)
points(0, 0, cex=15, col = "blue", pch = 10)
qqnorm(nyse)
abline(a = 0, b = 0.008, col = "red", lwd = 3)
points(0, 0, cex=5, col = "blue", pch = 20)
qqnorm(nyse)
abline(a = 0, b = 0.008, col = "red", lwd = 3)
points(0, 0, cex=2, col = "blue", pch = 20)
plot(soi, rec, type = "o")
plot(soi, rec, pch = 20)
plot(soi, rec, pch = 20, col = "red")
plot(soi, rec, pch = 20, col = "red", xlab = "Southern Oscillation Index", ylab = "Recruitment")
?runif
?rnorm
n = 100
w = rnorm(n)
x = cumsum(w)
plot(1:n, x)
plot(1:n, x, type = "o")
n = 1000
w = rnorm(n)
x = cumsum(w)
plot(1:n, x, type = "o")
abline(a = 0, b = 0, col = "red", type = ":")
abline(a = 0, b = 0, col = "red", type = "-")
abline(a = 0, b = 0, col = "red", type = ".")
?abline
n = 1000
w = rnorm(n)
x = cumsum(w)
plot(1:n, x, pch = 20)
plot(1:n, x, pch = 20, cex = 1)
n = 1000
w = rnorm(n)
x = cumsum(w)
plot(1:n, x, pch = 20, cex = 1)
plot(1:n, x, pch = 20, cex = 0.5)
plot(1:n, x, pch = 20, cex = 0.3)
plot(1:n, x, pch = 20, cex = 0.3)
plot(1:n, x, pch = 20, cex = 0.2)
plot(1:n, x, pch = 20, cex = 0.1)
n = 1000
w = rnorm(n)
x = cumsum(w)
plot(1:n, x, pch = 20, cex = 0.1)
set.seed(1)
n = 1000
w = rnorm(n)
x = cumsum(w)
plot(1:n, x, pch = 20, cex = 0.1)
abline(a = 0, b = 0, col = "red", type = 2)
abline(a = 0, b = 0, col = "red", type = "dotted")
abline(a = 0, b = 0, col = "red", lty = 2)
x = cumsum(w)
plot(1:n, x, pch = 20, cex = 0.1)
abline(a = 0, b = 0, col = "red", lty = 2)
set.seed(1)
n = 5000
w = rnorm(n)
x = cumsum(w)
plot(1:n, x, pch = 20, cex = 0.1)
abline(a = 0, b = 0, col = "red", lty = 2)
set.seed(10)
n = 5000
w = rnorm(n)
x = cumsum(w)
plot(1:n, x, pch = 20, cex = 0.1)
abline(a = 0, b = 0, col = "red", lty = 2)
respondents = c(29/36, 7/36)
as.string(respondents)
as.char(respondents)
as.chars(respondents)
as.character(respondents)
df = data.frame(Study = "infas and DLR, 2010", Sample = "N = 60,713; representative for
German population (MiD)", Results = "On average, people in Germany travel 39 km per day (oneday travel diary)." )
df.rbind("Öko-Institut, 2011", "Analysis of data collected in MiD 2008 (infas and DLR, 2010)", "80% of vehicles travel less than 50 km per day, 95% less than 100 km. On average, 12 trips per year exceed a driving range of 160 km.")
?rbind
df = rbind(df, "Öko-Institut, 2011", "Analysis of data collected in MiD 2008 (infas and DLR, 2010)", "80% of vehicles travel less than 50 km per day, 95% less than 100 km. On average, 12 trips per year exceed a driving range of 160 km.")
df = data.frame(Study = "infas and DLR, 2010", Sample = "N = 60,713; representative for
German population (MiD)", Results = "On average, people in Germany travel 39 km per day (oneday travel diary)." )
df = rbind(df, c("Öko-Institut, 2011", "Analysis of data collected in MiD 2008 (infas and DLR, 2010)", "80% of vehicles travel less than 50 km per day, 95% less than 100 km. On average, 12 trips per year exceed a driving range of 160 km."))
df
df = data.frame(Study = "infas and DLR, 2010", Sample = "N = 60,713; representative for German population (MiD)", Results = "On average, people in Germany travel 39 km per day (oneday travel diary)." )
df = rbind(df, c("Öko-Institut, 2011", "Analysis of data collected in MiD 2008 (infas and DLR, 2010)", "80% of vehicles travel less than 50 km per day, 95% less than 100 km. On average, 12 trips per year exceed a driving range of 160 km."))
df = data.frame(Study = "infas and DLR, 2010", Sample = "N = 60,713; representative for German population (MiD)", Results = "On average, people in Germany travel 39 km per day (oneday travel diary)." )
df = rbind(df, c("Öko-Institut, 2011", "Analysis of data collected in MiD 2008 (infas and DLR, 2010)", "80% of vehicles travel less than 50 km per day, 95% less than 100 km. On average, 12 trips per year exceed a driving range of 160 km."), stringsAsFactors = FALSE)
df = data.frame(Study = "infas and DLR, 2010", Sample = "N = 60,713; representative for German population (MiD)", Results = "On average, people in Germany travel 39 km per day (oneday travel diary).", stringsAsFactors = FALSE)
df = rbind(df, stringsAsFactors = FALSE, c("Öko-Institut, 2011", "Analysis of data collected in MiD 2008 (infas and DLR, 2010)", "80% of vehicles travel less than 50 km per day, 95% less than 100 km. On average, 12 trips per year exceed a driving range of 160 km."))
df
df = data.frame(Study = "infas and DLR, 2010", Sample = "N = 60,713; representative for German population (MiD)", Results = "On average, people in Germany travel 39 km per day (oneday travel diary).", stringsAsFactors = FALSE)
df = rbind(df, c("Öko-Institut, 2011", "Analysis of data collected in MiD 2008 (infas and DLR, 2010)", "80% of vehicles travel less than 50 km per day, 95% less than 100 km. On average, 12 trips per year exceed a driving range of 160 km."))
df
library(XML)
u = "http://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
tt = readHTMLTable(u)
class(tt)
length(tt)
sapply(tt, class)
head(tt[[1]])
str(tt[[1]])
tt = readHTMLTable(u, which = 1, stringsAsFactors = FALSE) # get the 1st table the function returns
head(tt)
sapply(tt, class)
as.integer(gsub(",", "", head(tt[[3]])))
tt[[3]]
tt$population
tt$Population
as.integer(gsub(",", "", head(tt$Population)))
tt = readHTMLTable(u, which = 1, stringsAsFactors = FALSE, colClasses = list("integer", "character", "FormattedInteger", "character", "Percent", NULL)) # get the 1st table the function returns
sapply(tt, class)
dim(tt)
?readHTMLTable
ll = getHTMLLinks(u)
ll
u1 = "http://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
tt = readHTMLTable(u1, which = 1, stringsAsFactors = FALSE)) # get the 1st table the function returns
tt = readHTMLTable(u1, which = 1, stringsAsFactors = FALSE) # get the 1st table the function returns
dim(tt)
head(tt)
u2 = "http://www.city-data.com/city/California.html"
ll = getHTMLLinks(u2)
head(ll)
doc = htmlParse(u2)
doc
class(doc)
library(RCurl)
getNodeSet(doc, "//table//a[@href = 'Davis-California.html']")
u3 = "http://www.kaggle.com/jobs"
a = xmlValue(getNodeSet(doc2, "//table//a[@href = 'Davis-California.html']/../../../td[3]/b/text()")[[1]]) #..: go to parent
doc2 = htmlParse(u2)
a = xmlValue(getNodeSet(doc2, "//table//a[@href = 'Davis-California.html']/../../../td[3]/b/text()")[[1]]) #..: go to parent
a
a = xmlValue(getNodeSet(doc2, "//table//a[@href = 'Davis-California.html']/../../../td[3]")[[1]]) #..: go to parent
a
class(a)
a = getNodeSet(davis, "//text()[contains(., 'Males:')]/../..")[[1]]
u3 = "http://www.city-data.com/city/Davis-California.html"
davis = htmlParse(u3)
a = getNodeSet(davis, "//text()[contains(., 'Males:')]/../..")[[1]]
a
a[[1]]
a[[2]]
xmlValue(a[[2]])
a = getNodeSet(davis, "//text()[contains(., 'Males:')]/../../../../../..")[[1]]
a
a = getNodeSet(davis, "//section[@id = 'population-by-sex']//tr/td[1]/text()")
a
a = xpathSApply(getNodeSet(davis, "//section[@id = 'population-by-sex']//tr/td[1]/text()"), xmlValue)
a
a2 = xpathSApply(davis, "//section[@id = 'population-by-sex']//tr/td[1]/text()", xmlValue)
a2
getNodeSet(davis, "//section[@id = 'population-by-sex']//tr")
set.seed(10)
w = rnorm(150, 0, 1)
x = filter(w, filter = c(0, -0.9), method = "recursive")[-(1:50)]
plot.ts(x, main = "Autoregression")
?filter
v = filter(w, sides = 1, rep(1/4, 4))
lines(v, lty = 2, col = "red")
v = filter(w, sides = 1, rep(1/4, 4))[-(1:50)]
lines(v, lty = 2, col = "red")
set.seed(10)
w = rnorm(150, 0, 1)
x = filter(w, filter = c(0, -0.9), method = "recursive")[-(1:50)]
plot.ts(x, main = "Autoregression")
v = filter(w, sides = 1, rep(1/4, 4))[-(1:50)]
lines(v, lty = 2, col = "red")
w = c(1:10)
filter(w, sides = 1, rep(1/4, 4))
set.seed(10)
w = rnorm(150, 0, 1)
x = filter(w, filter = c(0, -0.9), method = "recursive")[-(1:50)]
plot.ts(x, main = "Autoregression", lwd = 3)
v = filter(w, sides = 1, rep(1/4, 4))[-(1:50)]
lines(v, lty = 2, col = "red", lwd = 3)
lines(v, lty = 3, col = "red", lwd = 3)
plot.ts(x, main = "Autoregression", lwd = 3)
v = filter(w, sides = 1, rep(1/4, 4))[-(1:50)]
lines(v, lty = 3, col = "red", lwd = 3)
u5 = "http://www.rateinflation.com/consumer-price-index//usa-historical-cpi"
library(RCurl)
txt = getForm(u, 'start-year' = '1913', 'end-year' = '2000')
nchar(txt)
u5 = "http://www.rateinflation.com/consumer-price-index//usa-historical-cpi"
library(RCurl)
txt5 = getForm(u, 'start-year' = '1913', 'end-year' = '2000')
doc5 = htmlParse(txt5, asText = TRUE)
tt5 = readHTMLTable(doc)
head(tt5)
u5 = "http://www.rateinflation.com/consumer-price-index/usa-historical-cpi"
library(RCurl)
txt5 = getForm(u, 'start-year' = '1913', 'end-year' = '2000')
doc5 = htmlParse(txt5, asText = TRUE)
tt5 = readHTMLTable(doc)
dim(tt5)
head(tt5)
tt5
u5 = "http://www.rateinflation.com/consumer-price-index/usa-historical-cpi"
txt5 = getForm(u5, 'start-year' = '1913', 'end-year' = '2000')
doc5 = htmlParse(txt5, asText = TRUE)
tt5 = readHTMLTable(doc)
head(tt5)
tt5 = readHTMLTable(doc5)
tt5
?readHTMLTable
tt5 = readHTMLTable(doc5, header = TRUE)
TT5
tt5
tt5 = readHTMLTable(doc5, header = TRUE)
tt5
h = getCurlHandle(cookiejar = "", verbose = TRUE)
getURLContent("http://pems.dot.ca.gov", curl = h)
getURLContent("www.nytimes.com", curl = h)
getURLContent("http://www.nytimes.com", curl = h)
h = getCurlHandle(cookiejar = "", verbose = FALSE)
getURLContent("http://www.nytimes.com", curl = h)
h = getCurlHandle(cookiejar = "", verbose = TRUE)
getURLContent("http://www.nytimes.com", curl = h)
h = getCurlHandle(cookiejar = "", verbose = TRUE)
getURLContent("https://www.nytimes.com", curl = h)
getURLContent("http://www.nytimes.com", curl = h)
cookie = "__utma=158387685.1256988470.1427662016.1427674028.1427677756.3; __utmz=158387685.1427677756.3.3.utmcsr=webapp.ftb.ca.gov|utmccn=(referral)|utmcmd=referral|utmcct=/MyFTBAccess/iAccountInactive.aspx; PHPSESSID=17143b98bb1b323508125f3cbac8fa3e; __utmt=1; __utma=267661199.1843289294.1428182825.1428182825.1428182825.1; __utmb=267661199.10.10.1428182825; __utmc=267661199; __utmz=267661199.1428182825.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)"
u6 = "http://pems.dot.ca.gov/?report_form=1&dnode=State&content=detector_health&tab=dh_timeseries&export=&s_time_id=1420329600&s_mm=1&s_dd=4&s_yy=2015&e_time_id=1428105599&e_mm=4&e_dd=3&e_yy=2015&gn=week&q=pct&filter=all&st_cd=on&st_ch=on&st_ff=on&st_hv=on&st_ml=on&st_fr=on&st_or=on&view=line&eqpo=&gb=&html.x=44&html.y=10"
txt = getURLContent(u6, cookie = cookie, verbose = TRUE) ## verbose shows the request details
txt
cookie = "__utma=158387685.1256988470.1427662016.1427674028.1427677756.3; __utmz=158387685.1427677756.3.3.utmcsr=webapp.ftb.ca.gov|utmccn=(referral)|utmcmd=referral|utmcct=/MyFTBAccess/iAccountInactive.aspx; PHPSESSID=17143b98bb1b323508125f3cbac8fa3e; __utma=267661199.1843289294.1428182825.1428182825.1428182825.1; __utmb=267661199.11.10.1428182825; __utmc=267661199; __utmz=267661199.1428182825.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)"
u6 = "http://pems.dot.ca.gov/?report_form=1&dnode=State&content=detector_health&tab=dh_timeseries&export=&s_time_id=1420329600&s_mm=1&s_dd=4&s_yy=2015&e_time_id=1428105599&e_mm=4&e_dd=3&e_yy=2015&gn=week&q=pct&filter=all&st_cd=on&st_ch=on&st_ff=on&st_hv=on&st_ml=on&st_fr=on&st_or=on&view=line&eqpo=&gb=&html.x=44&html.y=10"
txt = getURLContent(u6, cookie = cookie, verbose = TRUE) ## verbose shows the request details
u6
txt = getURLContent(u6, cookie = cookie, verbose = TRUE)
cookie="__utma=158387685.1256988470.1427662016.1427674028.1427677756.3; __utmz=158387685.1427677756.3.3.utmcsr=webapp.ftb.ca.gov|utmccn=(referral)|utmcmd=referral|utmcct=/MyFTBAccess/iAccountInactive.aspx; PHPSESSID=17143b98bb1b323508125f3cbac8fa3e; __utmt=1; __utma=267661199.1843289294.1428182825.1428182825.1428182825.1; __utmb=267661199.13.10.1428182825; __utmc=267661199; __utmz=267661199.1428182825.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)"
cookie
txt = getURLContent(u6, cookie = cookie, verbose = TRUE)
?getURLContent
h = getCurlHandle(cookiejar = "", verbose = TRUE)
getURLContent(u6, curl = h)
txt6_1 = getURLContent(u6, cookie = cookie, verbose = TRUE) ## verbose shows the request details
h6 = getCurlHandle(cookiejar = "", verbose = TRUE)
txt6_2 = getURLContent(u6, curl = h6)
u7 = "http://climatedataapi.worldbank.org/climateweb/rest/v1/country/mavg/tas/2020/2039/USA.json"
j7 = getURLContent(u7)
j7
library(RJSONIO)
install.packages("RJSONIO")
library(RJSONIO)
a7 = fromJSON(j7, asText = TRUE)
str(a7)
xmlParse(txt6_2, asText = TRUE)
type(txt6_2)
class(txt6_2)
txt6_1 == txt6_2
txt6_1
htmlParse(txt6_1)
?readHTMLTable
readHTMLTable(htmlParser(txt6_1))
readHTMLTable(htmlParse(txt6_1))
tt6 = readHTMLTable(htmlParse(txt6_1))
dim(tt6)
length(tt6)
class(tt6)
tt6[1]
tt6[2]
tt6[3]
tt6[4]
tt6[5]
tt6_2 = readHTMLTable(htmlParse(txt6_2))[[5]]
tt6_2
readHTMLTable(htmlParse(txt6_2))[[1]]
readHTMLTable(htmlParse(txt6_2))[[2]]
readHTMLTable(htmlParse(txt6_2))[[3]]
readHTMLTable(htmlParse(txt6_2))[[4]]
readHTMLTable(htmlParse(txt6_2))[[5]]
readHTMLTable(htmlParse(txt6_2))[[6]]
readHTMLTable(htmlParse(txt6_2))[[7]]
?getCurlHandle
df = data.frame(Parameter = "Range preferences", N = " ", M = " ", Mdn = " ", SD = " ", P25 = " ", P75 = " ")
df = rbind(df, c("Minimum acceptable range T1", "78", "135.9", "120.0", "60.1", "100.0", "155.0"))
df = rbind(df, c("Minimum acceptable range T0", "39", "144.9", "120.0", "59.6", "100.0", "150.0"))
df = rbind(df, c("Appropriate range T1", "78", "214.5", "200.0", "89.8", "150.0", "262.5"))
df = rbind(df, c("Range needs", "", "", "", "", "", ""))
df = rbind(df, c("\tM7D", "66", "60.1", "45.6", "38.3", "35.8", "66.7"))
df = rbind(df, c("\tMax7D", "66", "152.7", "83.5", "150.9", "63.8", "161.3"))
df = rbind(df, c("\tSumTD", "56", "46.7", "45.5", "26.0", "32.1", "55.1"))
df = rbind(df, c("CV performant Range", "39", "607.8", "600.0", "201.2", "480.0", "700.0"))
library(knitr)
df = data.frame(Method = "Direct", Study = "VDE, 2010", Sample = "N = 1,000; German residents >14 years of age", Results = "The average German resident considers 353 km driving range to be acceptable", stringsAsFactors = FALSE)
df = rbind(df, c("Direct", "ADAC, 2013", "N = 803 (2011) vs. 507 (2013) ADAC members.", "In 2011 74% of ADAC (German automobile club) members desired an EV range of more than 200 km. In 2013, this decreased to 50%"))
df = rbind(df, c("Direct", "Bunzeck et al., 2011", "European respondents require 308 km driving range on average; for German consumers, this figure is somewhat higher (328 km)"))
df = rbind(df, c("Direct", "Giffi et al., 2011", "N > 13,000 respondents from 17 countries worldwide", "60% of German respondents want an EV to have more than 320 km driving range before they would consider a purchase"))
df = rbind(df, c("Direct", "Bronchard et al., 2011", "N = 7003; representative for the general population of 12 countries worldwide", "In order to consider purchasing an EV, people worldwide would prefer to have a range of at least 437 km"))
df = rbind(df, c("Direct","Zpryme, 2010", "N = 1046; representative for US drivers", "On average, US drivers consider 294 miles driving range to be acceptable"))
df = rbind(df, c("Indirect", "Dimitropoulos et al., 2011", "Meta-analysis of 31 discrete choice and contingent ranking studies", "Compensating variation to extend driving range
from 100 to 150 miles is 3500 US$ (16,200 US$ from 100 up to 350 miles). Willingness to pay for one-mile increase in driving range between 47 and 64 US$"))
df = rbind(df, c("Indirect", "Daziano, 2013", "N = 500 California residents", "An EV is perceived as attractive as a gasoline vehicle if its driving range reaches 330 miles.
However, if operating costs are integrated into the analysis, the estimate decreases to 180 miles (assuming comparable prices of EV and gasoline vehicle)"))
df = rbind(df, c("Indirect", "Hoen & Koetse, 2012", "N = 1802 Dutch drivers", "Willingness to pay for an increase of driving range reaches its maximum in lower range areas, i.e., consumers are highly motivated to avoid low EV driving ranges"))
df
library(png)
install.packages("png")
library(grid)
library(png)
library(grid)
img1 <- readPNG(C:\Users\Jing\Desktop\SustainableMobility\Fig4.gif)
img2 <- readPNG(C:\Users\Jing\Desktop\SustainableMobility\Fig5.gif)
img3 <- readPNG(C:\Users\Jing\Desktop\SustainableMobility\Fig6.gif)
grid.raster(img1)
grid.raster(img2)
grid.raster(img3)
img1 <- readPNG('C:\Users\Jing\Desktop\SustainableMobility\Fig4.gif')
img2 <- readPNG('C:\Users\Jing\Desktop\SustainableMobility\Fig5.gif')
img3 <- readPNG('C:\Users\Jing\Desktop\SustainableMobility\Fig6.gif')
grid.raster(img1)
grid.raster(img2)
grid.raster(img3)
?readPNG
img1 <- readPNG(C:/Users/Jing/Desktop/SustainableMobility/Fig4.gif)
img1 <- readPNG('C:/Users/Jing/Desktop/SustainableMobility/Fig4.gif')
img1 <- readPNG('C:/Users/Jing/Desktop/SustainableMobility/Fig4.gif')
img1 <- readPNG('C:/Users/Jing/Desktop/SustainableMobility/Fig4.png')
pandoc("C:\Users\Jing\Desktop\SustainableMobility\Jing - Electric Vehicle Adoption&Travel Behavior.Rmd", format = "latex")
gwd()
getwd()
pandoc("C:/Users/Jing/Desktop/SustainableMobility/Jing - Electric Vehicle Adoption&Travel Behavior.Rmd", format = "latex")
library(knitr)
pandoc("C:/Users/Jing/Desktop/SustainableMobility/Jing - Electric Vehicle Adoption&Travel Behavior.Rmd", format = "latex")
pandoc("C:/Users/Jing/Desktop/SustainableMobility/Jing - Electric Vehicle Adoption&Travel Behavior.Rmd", format = "latex")
WBC
library(tsa)
library()
library(astsa)
WBC
blood = cbind(WBC, PLT, HCT)
blood = replace(boold, bolld==0, NA)
blood = replace(blood, blood==0, NA)
?replace
plot(blood, type = 'o', pch = 19, xlab = "day", main = "")
data(iris)
head(iris)
View(iris)
a = [1, 2, 3]
a = c(1, 2, 3)
aT = a.t
a.T()
a = matrix(c(1, 2, 3), 3, 1)
a
aT = t(a)
aT
outer(a, aT, FUN = '*')
a %*% aT
aaT <- a %*% aT
aaT %*% aaT
?read.dat
data <- read.txt(r'C:\Users\Jing\Desktop\MS&E 226\HW1\ARM_Data\electric.company\electric.dat', sep = ' ', header = T)
data <- read.txt('C:/Users/Jing/Desktop/MS&E 226/HW1/ARM_Data/electric.company/electric.dat', sep = ' ', header = T)
?read.txt
data <- read.table('C:/Users/Jing/Desktop/MS&E 226/HW1/ARM_Data/electric.company/electric.dat', sep = ' ', header = T)
?read.table
data <- read.table('C:/Users/Jing/Desktop/MS&E 226/HW1/ARM_Data/electric.company/electric.dat', sep = ' ', header = F)
data <- read.table('C:/Users/Jing/Desktop/MS&E 226/HW1/ARM_Data/electric.company/electric.dat', sep = '\t', header = F)
data
data <- read.table('C:/Users/Jing/Desktop/MS&E 226/HW1/ARM_Data/electric.company/electric.dat', sep = ' ', header = T)
url <- 'C:/Users/Jing/Desktop/MS&E 226/HW1/ARM_Data/electric.company/electric.dat'
readLines(url)
data <- read.table(url, sep = '\t', header = T)
data
data <- read.table(url, sep = '\s', header = T)
data <- read.table(url, sep = ' ', header = T)
data <- read.table(url, sep = ' ', header = F)
data <- read.table(url, sep = ' ', header = F, skip = 1)
data <- read.table(url, sep = ' ', header = F, skip = 2)
data <- read.table(url, header = T)
data
sgroup <- data[data$Supplement == 'S', ]
sgroup
plot(sgroup$treated.Posttest, sgroup$control.Posttest)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch='*')
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch='.', cex = 2)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch='.', cex = 5)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch=15, cex = 5)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch=15, cex = 1)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch=16, cex = 1)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch=16)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch = 16, col = 'red')
mean(sgroup$treated.Posttest - sgroup$control.Posttest)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch = 16, col = 'red', main = "Supplement Group")
rgroup <- data[data$Supplement == 'R', ]
plot(rgroup$treated.Posttest, rgroup$control.Posttest, pch = 16, col = 'red', main = "Replacement Group")
mean(rgroup$treated.Posttest - rgroup$control.Posttest)
plot(sgroup$treated.Posttest, sgroup$control.Posttest, pch = 16, col = 'red', main = "Supplement Group")
abline(0, 1)
abline(0, 1, cex = 2)
abline(0, 1, lwd = 2)
abline(0, 1, lwd = 3)
rgroup <- data[data$Supplement == 'R', ]
plot(rgroup$treated.Posttest, rgroup$control.Posttest, pch = 16, col = 'red', main = "Replacement Group")
abline(0, 1, lwd = 3)
sum(sgroup$control.Posttest > sgroup$treated.Posttest)
sum(rgroup$control.Posttest > rgroup$treated.Posttest)
24/61
7/35
data
a = matrix(c(3, 2, 6), 3, 1)
a
at = t(a)
at
d = a %*% at/(at %*% a)
d = a %*% at
denom = at %*% a
denom
d/denom[1, 1]
d1 = d/denom[1, 1]
d1
d1 %*% d1
d
x = matrix(c(2, 3, 6), 3, 1)
xT = t(x)
dividend = x %*% xT
divisor = xT %*% x
d = dividend/divisor[1, 1]
d%*%d
dividend
d
getwd()
this.dir <- dirname(parent.frame(2)$ofile)
source(..., chidir = T)
source('my-solution.R', chidir = T)
source(my-solution.R, chidir = T)
source(my-solution.R, chdir = T)
source('my-solution.R', chdir = T)
setwd("C:/Users/Jing/Desktop/dplyr tutorial")
flights <- read.csv('data/flights.csv', head = T)
flights$date <- as.Date(flights$date)
library(dplyr)
library(dplyr)
## setwd() to the correct directory
flights <- read.csv('data/flights.csv', head = T)
flights$date <- as.Date(flights$date)
planes <- read.csv('data/planes.csv', head = T)
weather <- read.csv('data/weather.csv', head = T)
weather$date <- as.Date(weather$date)
airports <- read.csv('data/airports.csv', head = T)
## Exercise 1: Find all flights
# To SFO or OAK
df1 <- filter(flights, dest %in% c('SFO', 'OAK'))
# In January
library(lubridate)
flights <- mutate(flights, year = year(flights$date), month = month(flights$date), day = day(flights$date))
df2 <- filter(flights, month == 1)
# Delayed by more than an hour
df3 <- filter(flights, dep_delay > 60)
# That departed between midnight and five am
df4 <- filter(flights, hour >= 0, hour <= 5)
## Exercise 2:
# Order the flights by depature date and time
df5 <- arrange(flights, date, hour, minute)
# Which flights were most delayed
df6 <- arrange(flights, desc(arr_delay))
df6$flight[1]
## Exercise 3:
# Compute speed in mph from time and distance. Which flight flew the fastest
df7 <- mutate(flights, mph = dist/(time/60))
df7 <- arrange(df7, desc(mph))
df7$flight[1]
# Compute hour and minute from dep
df8 <- mutate(flights, hr = dep %/% 100, min = dep %% 100)
## Exercise 4:
# Which destinations have the highest average delays?
df9 <- flights %>%
filter(!is.na(arr_delay)) %>%
group_by(dest) %>%
summarise(avg_delay = mean(arr_delay), n = n()) %>%
filter(n > 10) %>%
arrange(desc(avg_delay)) %>%
head(5)
# Which flights (i.e. carrier + flight) happen every day? Where do they fly to?
num_date <- length(unique(flights$date))
df10 <- flights %>%
group_by(carrier, flight) %>%
summarise(count = n_distinct(date)) %>%
filter(count == num_date)
df11 <- transmute(df10, cf = paste(carrier, flight, sep = ''))
df12 <- flights %>%
mutate(cf = paste(carrier, flight, sep = '')) %>%
select(cf, dest) %>%
filter(cf %in% df11$cf) %>%
select(dest) %>%
distinct()
# On average, how do delays (of non-cancelled flights) vary over the course of a day?
library(ggplot2)
df13 <- flights %>%
filter(cancelled == 0) %>%
mutate(h = hour + minute/60) %>%
group_by(h) %>%
summarise(avg_delay = mean(dep_delay), n = n()) %>%
filter(n > 30) %>%
arrange(h)
ggplot(df13, aes(h, avg_delay)) +
geom_point(aes(size = n), alpha = 0.2) +
scale_size_area() +
geom_smooth(method = 'loess')
## Exercise 5:
# Are older planes more likely to be delayed?
by_year <- planes %>% select(plane, year) %>% filter(!is.na(year))
delay <- flights %>% group_by(plane) %>%
summarise(avg_delay = mean(arr_delay, na.rm = TRUE), count = n()) %>%
filter(count > 50) %>%
inner_join(by_year)
ggplot(delay, aes(year, avg_delay)) + geom_point() +
xlim(1990, 2011) + geom_smooth()
